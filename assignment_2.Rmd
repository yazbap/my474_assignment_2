---
title: "Assignment #2"
date: "February 26, 2024"
output: html_document
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(echo = FALSE)
```

## Exercise 1

What is the difference between train and train 2?

- Train 2 has an additional parameter `m` with the default set to 0.9. It also contains an additional vector `v` which has the same number of elements as coefficients in the model. These additional features turns this into a stochastic gradient descent with momentum algorithm. In these types of algorithms, the `v` vector stores the history of the magnitude of the parameter update equation for the kth coefficient which is done on the line `v[k] <- m*v[k] + l_rate*(yhat_i - y[i])*row_vec[k]`. The `m` parameter controls how much weight previous updates have on the current update. In this case, a value of 0.9 means that previous updates has a strong influence on the current update. 

How might this affect the performance of the algorithm?

To understand why adding momentum may be necessary, we first have to understand potential issues that can come up with SGD. A problem with SGD is that the progression toward the minima can move in inappropriate directions depending on the points sampled during the search which can can slow down progress. Adding previous update values as a factor can increase or decrease the magnitude of the next step and guide its direction. This can result in fewer steps reaching the minima. Momentum is helpful in cases where:

- the objective function has a lot of curvature meaning that the gradient may change a lot over small regions of the search space as the momentum will guide the algorithm with greater velocity toward the minima

- the gradient has a high variance as the momentum will provide more context on where the algorithm should go

- when the search space is flat or nearly flat as the momentum allows the search to progress in the same direction as before the flat spot and helpfully cross the flat region.

Demonstrate your argument by including a coded example, comparing this algorithm, and point to elements of the output that help illustrate your answer.

As shown in the plots for train_1, the algorithm we made in class and train_2, the algorithm given in this assignment, the second has more erratic behavior, while the former has a smoother line around the final MSE and NLL.

Critique the function more generally. What are some of the general constraints or limitations of the implementation? Are there any ways you could improve it further?

```{r train}
# Define the train function
train <- function(X, y, l_rate, epochs) {
  coefs <- rep(0, ncol(X))
  MSE_list <- vector("numeric", length = epochs)  # List to store MSE for each epoch
  NLL_list <- vector("numeric", length = epochs)  # List to store NLL for each epoch
  
  for (b in 1:epochs) {
    for (i in sample(1:nrow(X))) { 
      row_vec <- as.numeric(X[i,])
      
      yhat_i <- predict_row(row_vec, coefficients = coefs)
      
      # Update coefficients using gradient descent
      coefs <- sapply(1:length(coefs), function (k) {
        coefs[k] - l_rate * (yhat_i - y[i]) * row_vec[k]
      })
    }
    
    # Calculate current predictions
    yhat <- apply(X, 1, predict_row, coefficients = coefs)
    
    # Calculate MSE and NLL
    MSE_epoch <- MSE(y, yhat)
    NLL_epoch <- NLL(y, yhat)
    
    # Store MSE and NLL for this epoch
    MSE_list[b] <- MSE_epoch
    NLL_list[b] <- NLL_epoch
  }
  
  # Set up the multi-panel layout
  par(mfrow = c(1, 2))
  
  # Plot the MSE and NLL over epochs
  plot(1:epochs, MSE_list, type = "l", xlab = "Epoch", ylab = "MSE", main = "Train_1 MSE over Epochs")
  plot(1:epochs, NLL_list, type = "l", xlab = "Epoch", ylab = "NLL", main = "Train_1 NLL over Epochs")
  
  return(coefs)  # Output the final estimates
}
```


```{r train2}
# Define the train function
train2 <- function(X, y, l_rate, m = 0.9, epochs) {
  coefs <- rep(0, ncol(X))
  v <- rep(0, ncol(X)) # initialize v to 0
  MSE_list <- vector("numeric", length = epochs)  # List to store MSE for each epoch
  NLL_list <- vector("numeric", length = epochs)  # List to store NLL for each epoch
  
  for (b in 1:epochs) {
    for (i in sample(1:nrow(X))) { 
      row_vec <- as.numeric(X[i,])
      
      yhat_i <- predict_row(row_vec, coefficients = coefs)
      
      # Update coefficients using gradient descent
       for(k in 1:length(coefs)) { # loop over each coefficient
        
        v[k] <- m*v[k] + l_rate*(yhat_i - y[i])*row_vec[k] 
        
        coefs[k] <- coefs[k] - v[k] # update the coefficient
      }
    }
    
    # Calculate current predictions
    yhat <- apply(X, 1, predict_row, coefficients = coefs)
    
    # Calculate MSE and NLL
    MSE_epoch <- MSE(y, yhat)
    NLL_epoch <- NLL(y, yhat)
    
    # Store MSE and NLL for this epoch
    MSE_list[b] <- MSE_epoch
    NLL_list[b] <- NLL_epoch
    
  }
  
  # Set up the multi-panel layout
  par(mfrow = c(1, 2))
  
  # Plot the MSE and NLL over epochs
  plot(1:epochs, MSE_list, type = "l", xlab = "Epoch", ylab = "MSE", main = "Train_2 MSE over Epochs")
  plot(1:epochs, NLL_list, type = "l", xlab = "Epoch", ylab = "NLL", main = "Train_2 NLL over Epochs")
  
  return(coefs)  # Output the final estimates
}
```

```{r}
set.seed(89)

genX <- function(n) {
  return(
    data.frame(X0 = 1,
               X1 = runif(n,-5,5),
               X2 = runif(n,-2,2))
  )
}

genY <- function(X) {
  Ylin <- 3*X$X0 + 1*X$X1 - 2*X$X2 + rnorm(nrow(X),0,0.05) 
  Yp <- 1/(1+exp(-Ylin))
  Y <- rbinom(nrow(X),1,Yp)
  return(Y)
}

predict_row <- function(row, coefficients) {
  pred_terms <- row*coefficients # get the values of the individual linear terms
  yhat <- sum(pred_terms) # sum these up (i.e. \beta_0 + \beta_1X_1 + ...
  return(1/(1+exp(-yhat))) # convert to probabilities
}

X <- genX(1000)
y <- genY(X)
```


```{r, echo = TRUE}
coef_model <- train(X = X, y = y, l_rate = 0.01, epochs = 50)
```

```{r, echo = TRUE}
coef_model2 <- train2(X = X, y = y, l_rate = 0.01, m = 0.9, epochs = 50)
```


## Exercise 2

```{r}
mystery_function <- function(y, x){
  #function form
  # y = alpha + coefficient*x + lambda*R(f) 
  
  #Identify the optimal combination of both Î» and f in a principled way
  
  #Prints, as separate lines:
    # The optimal model form
    # The test loss of the optimal model
  
  # Returns the final trained model
}
```

```{r}
# Set seed for reproducibility
set.seed(123)

# Generate a vector of independent variable (X) with 100 observations
X <- rnorm(100)

# Generate the dependent variable (y) with a linear relationship to X, adding some random noise
# Assuming the linear relationship is y = 2*X + 3 + error
y <- 2*X + 3 + rnorm(100)

# Display the first few elements of X and y
head(cbind(X, y))

```


## Exercise 3

```{r}
load("civil_wars.RData")

View(civwars)
```

This document contains the necessary commands and layout to meet the formatting requirements for MY472. You should use the template.Rmd file as the basis for your own answers to the assigned exercises.

## Formatting requirements

For clarity, the formatting requirements for each assignment are:

* You must write in **full sentences**, as you would in a report or academic piece of writing
  
  * If the exercise requires generating a table or figure, you should include at least one sentence introducing and explaining it. E.g. "Table 1 reports the cross-validation test scores for a grid search of $\lambda$ values in the LASSO model."

* Unless stated otherwise, all code used to answer the exercises should be included as a code appendix at the end of the script. This formatting can be achieved by following the guidance in the template.Rmd file we provide on Moodle

* All code should be annotated with comments, to help the marker understand what you have done

* Your output should be replicable. Any result/table/figure that cannot be traced back to your code will not be marked

## Example of in-line figures without code

We achieve the formatting requirements in two-steps: 

  1) In the `setup` chunk, add `knitr::opts_chunk$set(echo = FALSE)` so that code is not included (echoed) by default in code chunks; 
  2) Add a specific code chunk at the end of the file to collect and print *all* the code in the Rmarkdown file. You can see this code at the bottom of this template. If you use this template for your assignments, do not delete the final code chunk!

Below, we use a code chunk to generate random data and include a scatter plot in-line. The code used to generate this chart is only reported at the end of the document. 

```{r plot_example}
set.seed(89) # set a seed for R's psuedo-randomiser, for replicability.
x <- rnorm(100) # randomly draw 100 obs from normal distribution, save as object
y <- rnorm(100) 
plot(x,y) # two-way scatterplot using R's default plotting
```

In specific instances, however, you may be directed to report your code in-line (or you may want to do this to illustrate a specific point). In these cases, we can override the default behaviour by adding the chunk option `echo = TRUE` to a specific R chunk. When `echo=TRUE`, your code is presented in-line with any output displayed afterwards. The same code will also be included in the appendix at the bottom of the document (which is fine).

```{r echo_example, echo=TRUE}
# {[language] [chunk_name], [chunk_options]}
# here we use echo=TRUE to override our global options and make the chunk appear exactly here. 

print("This code chunk is visible in this section.")
```

## Appendix: All code in this assignment

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE} 
# this chunk generates the complete code appendix. 
# eval=FALSE tells R not to run (``evaluate'') the code here (it was already run before).
```