---
title: "Assignment #2"
date: "February 26, 2024"
output: html_document
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(echo = FALSE)
library(glmnet)
```

## Exercise 1

### Algorithm Differences

`Train2()` differs from the `train` algorithm we considered in class in that it has an additional parameter `m` with the default set to 0.9. It also contains an additional vector `v` which has the same number of elements as coefficients in the model. These additional features turns this into a stochastic gradient descent with momentum algorithm. In these types of algorithms, the `v` vector stores the history of the magnitude of the parameter update equation for the kth coefficient which is done on the line `v[k] <- m*v[k] + l_rate*(yhat_i - y[i])*row_vec[k]`. The `m` parameter controls how much weight previous updates have on the current update. In this case, a value of 0.9 means that previous updates have a strong influence on the current update. 

To understand why adding momentum may be necessary, we first have to understand potential issues that can come up with SGD. A problem with SGD is that the progression toward the minima can move in inappropriate directions depending on the points sampled during the search which can can slow down progress. Adding previous update values as a factor can increase or decrease the magnitude of the next step and guide its direction. This can result in fewer steps reaching the minima. Momentum is helpful in cases where:

- The objective function has a lot of curvature meaning that the gradient may change a lot over small regions of the search space as the momentum will guide the algorithm with greater velocity toward the minima

- The gradient has a high variance, momentum will provide more context on where the algorithm should go

- When the search space is flat or, in other words, has a zero gradient, it typically indicates a local minimum (or maximum) of the function. If the optimization algorithm relies solely on the gradient, it might get stuck in such regions, unable to make progress towards finding the global minimum. Momentum allows the search to progress in the same direction as before the flat spot and ideally cross the flat region.

(Brownlee, 2021)

```{r train}
MSE <- function(ytrue, yhat) {
  return(mean((ytrue-yhat)^2))
}

NLL <- function(ytrue, yhat) {
  return(-sum(log(
    (yhat^ytrue)*((1-yhat)^(1-ytrue))
  )))
}

# Define the train function
train <- function(X, y, l_rate, epochs) {
  coefs <- rep(0, ncol(X))
  MSE_list <- vector("numeric", length = epochs)  # List to store MSE for each epoch
  NLL_list <- vector("numeric", length = epochs)  # List to store NLL for each epoch
  
  for (b in 1:epochs) {
    for (i in sample(1:nrow(X))) { 
      row_vec <- as.numeric(X[i,])
      
      yhat_i <- predict_row(row_vec, coefficients = coefs)
      
      # Update coefficients using gradient descent
      coefs <- sapply(1:length(coefs), function (k) {
        coefs[k] - l_rate * (yhat_i - y[i]) * row_vec[k]
      })
    }
    
    # Calculate current predictions
    yhat <- apply(X, 1, predict_row, coefficients = coefs)
    
    # Calculate MSE and NLL
    MSE_epoch <- MSE(y, yhat)
    NLL_epoch <- NLL(y, yhat)
    
    # Store MSE and NLL for this epoch
    MSE_list[b] <- MSE_epoch
    NLL_list[b] <- NLL_epoch
  }
  
  # Set up the multi-panel layout
  par(mfrow = c(1, 2))
  
  # Plot the MSE and NLL over epochs
  plot(1:epochs, MSE_list, type = "l", xlab = "Epoch", ylab = "MSE", main = "Figure 1: train() MSE over Epochs")
  plot(1:epochs, NLL_list, type = "l", xlab = "Epoch", ylab = "NLL", main = "Figure 2: train() NLL over Epochs")
  
  return(coefs)  # Output the final estimates
}
```


```{r train2}
# Define the train function
train2 <- function(X, y, l_rate, m = 0.9, epochs) {
  coefs <- rep(0, ncol(X))
  v <- rep(0, ncol(X)) # initialize v to 0
  MSE_list <- vector("numeric", length = epochs)  # List to store MSE for each epoch
  NLL_list <- vector("numeric", length = epochs)  # List to store NLL for each epoch
  
  for (b in 1:epochs) {
    for (i in sample(1:nrow(X))) { 
      row_vec <- as.numeric(X[i,])
      
      yhat_i <- predict_row(row_vec, coefficients = coefs)
      
      # Update coefficients using gradient descent
       for(k in 1:length(coefs)) { # loop over each coefficient
        
        v[k] <- m*v[k] + l_rate*(yhat_i - y[i])*row_vec[k] 
        
        coefs[k] <- coefs[k] - v[k] # update the coefficient
      }
    }
    
    # Calculate current predictions
    yhat <- apply(X, 1, predict_row, coefficients = coefs)
    
    # Calculate MSE and NLL
    MSE_epoch <- MSE(y, yhat)
    NLL_epoch <- NLL(y, yhat)
    
    # Store MSE and NLL for this epoch
    MSE_list[b] <- MSE_epoch
    NLL_list[b] <- NLL_epoch
    
  }
  
  # Set up the multi-panel layout
  par(mfrow = c(1, 2))
  
  # Plot the MSE and NLL over epochs
  plot(1:epochs, MSE_list, type = "l", xlab = "Epoch", ylab = "MSE", main = "Figure 3: train2() MSE over Epochs")
  plot(1:epochs, NLL_list, type = "l", xlab = "Epoch", ylab = "NLL", main = "Figure 4: train2() NLL over Epochs")
  
  return(coefs)  # Output the final estimates
}
```

```{r}
set.seed(89)

genX <- function(n) {
  return(
    data.frame(X0 = 1,
               X1 = runif(n,-5,5),
               X2 = runif(n,-2,2))
  )
}

genY <- function(X) {
  Ylin <- 3*X$X0 + 1*X$X1 - 2*X$X2 + rnorm(nrow(X),0,0.05) 
  Yp <- 1/(1+exp(-Ylin))
  Y <- rbinom(nrow(X),1,Yp)
  return(Y)
}

predict_row <- function(row, coefficients) {
  pred_terms <- row*coefficients # get the values of the individual linear terms
  yhat <- sum(pred_terms) # sum these up (i.e. \beta_0 + \beta_1X_1 + ...
  return(1/(1+exp(-yhat))) # convert to probabilities
}

X <- genX(1000)
y <- genY(X)
```

### Error Behavior

As shown in Figures 1 and 2, `train()` is quickly able to lower both the Mean squared error (MSE) and Negative Log Likelihood (NLL) very dramaically within the first 10 epochs, then tends to stay within a relatively small interval which helps us come to the conclusion of what the minimum values of these errors are. In showing this plot, `train()` shows us how SGD makes larger steps toward the minima when it is farther away and gradually takes smaller steps as we approach the ideal coefficients for our model. 

Figures 3 and 4 show the behavior of the errors for `train2()` which is much more erratic than `train()`. This shows us that momentum may bypass the ideal points for the coefficients and ultimately lead to a longer time needed for training, or we can also say that momentum should be used with only certain kinds of data. 

```{r, echo = TRUE}
coef_model <- train(X = X, y = y, l_rate = 0.01, epochs = 50)
```

```{r, echo = TRUE}
coef_model2 <- train2(X = X, y = y, l_rate = 0.01, m = 0.9, epochs = 50)
```

### General Constraints

In `train2()`, the amount of the momentum that contributes to the algorithm in each epoch is the same. If we vary how much momentum contributes to the next step, given how much loss was in the last epoch or weighing more recent updates to the parameter heavier than less recent ones, we can attain a more appropriate update size for the parameter. 

Furthermore, momentum can increase the complexity and memory requirements of the optimization, as it adds an extra term and hyperparameter that need to be stored and updated.

## Exercise 2

```{r}
ridgeLoss <- function(ytrue, yhat, coefficients, lambda) {
  nll <- NLL(ytrue, yhat)
  l2_penalty <- lambda*sum(coefficients^2)
  return(nll + l2_penalty)
}

set.seed(89)

genX <- function(n) {
  return (rnorm(n,0,1))
}

genY <- function(X) {
  Ylin <- 3*X + rnorm(length(X),0,0.5) 
  return(Ylin)
}

X <- genX(1000)
y <- genY(X)
```


```{r echo = TRUE}
#set lambda values to loop through
lambdas <- c(0.0001, 0.001, 0.01, 0.1, 1, 10)

#set polynomial degrees to loop through
poly_degrees <- seq(1, 7, by=1)

#set the number of folds
K <- 10

train_optimal_model <- function(x,y){
  # Initialize best lambda, degree, model and test loss to NULL to store optimal parameters and test loss
  best_lambda <- NULL
  best_degree <- NULL
  best_model <- NULL
  best_loss <- Inf
  
  #for each lambda value
  for (lambda in lambdas){
    
    #for each polynomial degree
    for (degree in poly_degrees){
      #transform the x data to this particular polynomial
      x_poly <- poly(x, degree)
  
      #assign each observation to a fold
      fold_id <- sample(rep(1:K, each = length(x_poly)/K))
      
      #set the total loss to 0 for this loop
      total_loss <- 0
      
      #for each fold
      for (k in 1:K){
        #assign the validation data for X
        val_X <- x_poly[fold_id == k]
        
        #assign the training data for X
        train_X <- x_poly[fold_id != k]
  
        #assign the validation data for y
        val_y <- y[fold_id == k]
        
        #assign the training data for y
        train_y <- y[fold_id != k]
        
        val_X <- val_X[!is.na(val_y)]
        train_X <- train_X[!is.na(train_y)]
        val_y <- val_y[!is.na(val_y)]
        train_y <- train_y[!is.na(train_y)]
  
        #create a model with this particular combination of polynomial and lambda using ridge regression
        k_mod <- glmnet(cbind(0,train_X), train_y, family = "gaussian", lambda = lambda, alpha = 0)
        
        # Create a constant variable for the second variable
        constant_value <- 0  # Adjust as needed
        
        # Repeat the constant value to match the length of val_X
        constant_variable <- rep(constant_value, length(val_X))
        
        # Combine the constant variable with val_X
        val_X_with_constant <- cbind(val_X, constant_variable)
  
        #make predictions with the model
        yhat_k <- predict(k_mod, newx = as.matrix(val_X_with_constant), type = "response")
  
        #calculate the loss
        k_loss <- ridgeLoss(val_y, yhat_k, coef(k_mod)[-1], lambda)
  
        #if the loss is less than the best test loss
        if (k_loss < best_loss){
          #set the best loss as this value
          best_loss <- k_loss
          
          #set the best degree to this degree
          best_degree <- degree
          
          #set the best lambda to this lambda
          best_lambda <- lambda
          
          #set the best model as this model
          best_model <- k_mod
        }
      }
      
    }
  }
      
    #set the model with all the data
    best_model <- glmnet(cbind(poly(x, best_degree), 0), 
                         y, 
                         family = "gaussian", 
                         lambda = best_lambda, 
                         alpha = 0)
  
    #Print the optimal model
    cat("The best model is: ", "glmnet(x = poly(x, ", 
        best_degree, "), y = y, family = gaussian, alpha = 0, lambda = ", 
        best_lambda, ")", 
        sep="" )
  
    #Print the test loss of the optimal model
    cat("\nThe loss of the optimal model is: ", best_loss)
  
    #return the model with the lowest loss
    return(best_model)
}

optimal_model <- train_optimal_model(X, y)

```

```{r}
set.seed(123)
x1 <- rnorm(100, mean = 10, sd = 2)
y1 <- 2 * x1 + rnorm(100, mean = 0, sd = 2)

x2 <- seq(-10, 10, by = 0.2)
y2 <- x2^2 + rnorm(length(x2), mean = 0, sd = 5)

x3 <- seq(0, 2 * pi, length.out = 100)
y3 <- sin(x3) + rnorm(length(x3), mean = 0, sd = 0.2)
```

```{r}
#y_pred <- predict(optimal_model, newx = as.matrix(x1))

#y_mean <- mean(y1)

# Compute mean squared error (MSE)
#mse <- mean((y1 - y_pred)^2)

# Compute mean absolute error (MAE)
#mae <- mean(abs(y1 - y_pred))

# Compute R-squared
#y_mean <- mean(y1)
#ss_total <- sum((y1 - y_mean)^2)
#ss_residual <- sum((y1 - y_pred)^2)
#r_squared <- 1 - (ss_residual / ss_total)

# Print the evaluation metrics
#cat("Mean Squared Error (MSE): ", mse, "\n")
#cat("Mean Absolute Error (MAE): ", mae, "\n")
#cat("R-squared: ", r_squared, "\n")
```


## Exercise 3

We have provided you with a dataset called civil_wars.RData, which records for each country and every year, whether that country was engaged in a civil war. The data was taken from Kaufman et al’s (2019) replication materials, which focuses on the use of boosted decision trees:

Kaufman AR, Kraft P, Sen M. Improving Supreme Court Forecasting Using Boosted Decision Trees. Political Analysis. 2019;27(3):381-387. doi:10.1017/pan.2018.59

You will see the variable descriptions under the variable names. We have removed country names from the dataset intentionally, so that all variables are numerical, and imputed any missing values.

Your task is to build a model to predict civil war. You should use the data however you see fit, and you may use any class of model we have considered (including any already covered in this assignment).

Your answer should be a report of between 250-500 words that a) introduces the problem, b) summarises any decisions you made about training the model, and c) demonstrate your final trained model’s performance. You may use (well-formatted) charts to help illustrate your claims.

Note: you do not need to submit the trained model object, and may use any R packages/functions in this task.

The models we learned in class so far are KNN, Trees and LASSO and Ridge regression. KNN would not be good in this case because the model because it does not typically work well with data with high dimensions. Trees also would not work because they are rarely the best in terms of prediction. 

Is LASSO or ridge better? In this case LASSO is the best model as assessed by a paper by Ward, Greenhill and Bakke Reference Ward, Greenhill and Bakke (2010) in which they state ..."when it comes to choosing a model that best predicts the occurrence of civil war, a very parsimonious model can often fare better than one that contains a relatively large number of statistically-significant variables."

```{r}
load("civil_wars.RData")

predict_civ_war <- function(civwar){
  #set the training index
  train_idx <- sample(1:nrow(civwar), 0.8*nrow(civwar))
  
  #set xtrain
  Xtrain <- civwar[train_idx,]
  #set x test
  Xtest <- civwar[-train_idx,]
  
  #set y train
  ytrain <- Xtrain$war
  #set y test
  ytest <- Xtest$war
  
  #remove same column from multiple data frames
  Xtrain$ViolentCrimesPerPop <- Xtest$ViolentCrimesPerPop <- NULL
  
  # use cv.glmt to estimate the best value for lambda
  lasso_cv <- cv.glmnet(as.matrix(Xtrain), ytrain, alpha = 1)
  
  #get the value of lambda
  lambda_min <- lasso_cv$lambda.min
  print(lambda_min)
  
  #train final model
  final_mod <- glmnet(as.matrix(Xtrain), ytrain, alpha = 1, lambda = lambda_min)
  
  return(final_mod)
}

prediction <- predict_civ_war(civwars)
```

This document contains the necessary commands and layout to meet the formatting requirements for MY472. You should use the template.Rmd file as the basis for your own answers to the assigned exercises.

## Formatting requirements

For clarity, the formatting requirements for each assignment are:

* You must write in **full sentences**, as you would in a report or academic piece of writing
  
  * If the exercise requires generating a table or figure, you should include at least one sentence introducing and explaining it. E.g. "Table 1 reports the cross-validation test scores for a grid search of $\lambda$ values in the LASSO model."

* Unless stated otherwise, all code used to answer the exercises should be included as a code appendix at the end of the script. This formatting can be achieved by following the guidance in the template.Rmd file we provide on Moodle

* All code should be annotated with comments, to help the marker understand what you have done

* Your output should be replicable. Any result/table/figure that cannot be traced back to your code will not be marked

## Example of in-line figures without code

We achieve the formatting requirements in two-steps: 

  1) In the `setup` chunk, add `knitr::opts_chunk$set(echo = FALSE)` so that code is not included (echoed) by default in code chunks; 
  2) Add a specific code chunk at the end of the file to collect and print *all* the code in the Rmarkdown file. You can see this code at the bottom of this template. If you use this template for your assignments, do not delete the final code chunk!

Below, we use a code chunk to generate random data and include a scatter plot in-line. The code used to generate this chart is only reported at the end of the document. 

```{r plot_example}
set.seed(89) # set a seed for R's psuedo-randomiser, for replicability.
x <- rnorm(100) # randomly draw 100 obs from normal distribution, save as object
y <- rnorm(100) 
plot(x,y) # two-way scatterplot using R's default plotting
```

In specific instances, however, you may be directed to report your code in-line (or you may want to do this to illustrate a specific point). In these cases, we can override the default behaviour by adding the chunk option `echo = TRUE` to a specific R chunk. When `echo=TRUE`, your code is presented in-line with any output displayed afterwards. The same code will also be included in the appendix at the bottom of the document (which is fine).

```{r echo_example, echo=TRUE}
# {[language] [chunk_name], [chunk_options]}
# here we use echo=TRUE to override our global options and make the chunk appear exactly here. 

print("This code chunk is visible in this section.")
```

## References

Brownlee J. (2021, October 12). Gradient Descent With Momentum from Scratch. Machine Learning Mastery. https://machinelearningmastery.com/gradient-descent-with-momentum-from-scratch/

## Appendix: All code in this assignment

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE} 
# this chunk generates the complete code appendix. 
# eval=FALSE tells R not to run (``evaluate'') the code here (it was already run before).
```